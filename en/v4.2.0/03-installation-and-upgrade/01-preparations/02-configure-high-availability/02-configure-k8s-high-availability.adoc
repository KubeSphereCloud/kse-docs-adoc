---
title: "Configuring Kubernetes High Availability"
keywords: "Kubernetes, {ks_product}, Installation, Preparation, High Availability"
description: "Describes how to configure multiple control plane nodes for a KubeSphere cluster in a production environment."
weight: 02
---


This section describes how to configure multiple control plane nodes for a KubeSphere cluster in a production environment to prevent cluster service interruption in case of a single control plane node failure, thereby achieving high availability. If your KubeSphere cluster does not have high availability requirements, you can skip this section.

This section introduces the following high availability configuration methods:

* Using local load balancing configuration. During the installation of KubeSphere, you can configure the KubeKey tool to install HAProxy on the worker nodes as a reverse proxy for each control plane node. All Kubernetes components on the worker nodes will connect to each control plane node through HAProxy. This method requires additional health check mechanisms, so its operational efficiency is somewhat reduced compared to other methods, but it can be used in scenarios without a dedicated load balancer and with a limited number of servers.

* Using a dedicated load balancer. You can use a load balancer provided by the cloud environment as a reverse proxy for each control plane node. This method requires the KubeSphere cluster to be installed in a cloud environment, and the cloud environment must be able to provide a dedicated load balancer.

* Using general-purpose servers as load balancers. You can install Keepalived and HAProxy on Linux servers outside the cluster nodes as load balancers. This method requires at least 2 additional Linux servers.

== Using Local Load Balancing Configuration

To achieve high availability using HAProxy, you only need to set the following parameters in the installation configuration file **config-sample.yaml** when installing KubeSphere:

// YAML
[source,yaml]
----
spec:
  controlPlaneEndpoint:
    internalLoadbalancer: haproxy
    domain: lb.kubesphere.local
    address: ""
    port: 6443
----

KubeKey will automatically install HAProxy on the worker nodes and complete the high availability configuration. You do not need to perform any other operations. For more information, please refer to link:../../../02-install-kubesphere/[Installing KubeSphere].

== Using a Dedicated Load Balancer
To achieve high availability using a dedicated load balancer provided by the cloud environment, you need to perform the following operations in the cloud environment:

. Create a load balancer with at least two replicas in the cloud environment.

. Configure the load balancer to listen on port 6443 of each control plane node in the KubeSphere cluster.

. Obtain the IP address of the load balancer for use during the subsequent installation of KubeSphere.

For specific operations, please refer to the user guide of your cloud environment or contact your cloud service provider.

== Using General-Purpose Servers as Load Balancers
The following describes how to configure general-purpose servers as load balancers using Keepalived and HAProxy.


=== Prerequisites

* You need to prepare 2 Linux servers that belong to the same private network as the cluster nodes to be used as load balancers.

* You need to prepare a virtual IP address (VIP) to be used as the floating IP address for the 2 load balancer servers. This address should not be occupied by other devices or components to avoid address conflicts.


=== Configuring High Availability

. Log in to the server to be used as a load balancer and execute the following command to install HAProxy and Keepalived (the following uses the Ubuntu operating system as an example; for other operating systems, replace **apt** with the corresponding package management tool):
+
--
[source,bash]
----
apt install keepalived haproxy psmisc -y
----
--

. Execute the following command to edit the HAProxy configuration file:
+
--
// Bash
[source,bash]
----
vi /etc/haproxy/haproxy.cfg
----
--

. Add the following information to the HAProxy configuration file and save the file (replace <IP address> with the private IP addresses of each control plane node in the KubeSphere cluster):
+
--
// Bash
[source,bash]
----
global
    log /dev/log  local0 warning
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon
   
   stats socket /var/lib/haproxy/stats
   
defaults
  log global
  option  httplog
  option  dontlognull
        timeout connect 5000
        timeout client 50000
        timeout server 50000
   
frontend kube-apiserver
  bind *:6443
  mode tcp
  option tcplog
  default_backend kube-apiserver
   
backend kube-apiserver
    mode tcp
    option tcplog
    option tcp-check
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
    server kube-apiserver-1 <IP address>:6443 check
    server kube-apiserver-2 <IP address>:6443 check
    server kube-apiserver-3 <IP address>:6443 check
----
--

. Execute the following command to restart HAProxy:
+
--
// Bash
[source,bash]
----
systemctl restart haproxy
----
--

. Execute the following command to set HAProxy to start automatically after boot:
+
--
// Bash
[source,bash]
----
systemctl enable haproxy
----
--

. Execute the following command to edit the Keepalived configuration file:
+
--
// Bash
[source,bash]
----
vi /etc/keepalived/keepalived.conf
----
--

. Add the following information to the Keepalived configuration file and save the file:
+
--
// Bash
[source,bash]
----
global_defs {
  notification_email {
  }
  router_id LVS_DEVEL
  vrrp_skip_check_adv_addr
  vrrp_garp_interval 0
  vrrp_gna_interval 0
}
   
vrrp_script chk_haproxy {
  script "killall -0 haproxy"
  interval 2
  weight 2
}
  
vrrp_instance haproxy-vip {
  state BACKUP
  priority 100
  interface <NIC>
  virtual_router_id 60
  advert_int 1
  authentication {
    auth_type PASS
    auth_pass 1111
  }
  unicast_src_ip <source IP address>
  unicast_peer {
    <peer IP address>
  }
  
  virtual_ipaddress {
    <floating IP address>
  }
  
  track_script {
    chk_haproxy
  }
}
----

Replace the following parameters with actual values:

[%header,cols="1a,2a"]
|===
|Parameter |Description

|<NIC>
|The network interface card name of the current load balancer.

|<source IP address>
|The IP address of the current load balancer.

|<peer IP address>
|The IP address of the other load balancer.

|<floating IP address>
|The virtual IP address to be used as the floating IP address.
|===
--

. Execute the following command to restart Keepalived:
+
--
// Bash
[source,bash]
----
systemctl restart keepalived
----
--

. Execute the following command to set Keepalived to start automatically after boot:
+
--
// Bash
[source,bash]
----
systemctl enable keepalived
----
--

. Repeat the above steps to install and configure HAProxy and Keepalived on the other load balancer server.

. Record the floating IP address for use during the subsequent installation of KubeSphere.


=== Verifying High Availability

. Log in to the first load balancer server and execute the following command to view the floating IP address:
+
--
// Bash
[source,bash]
----
ip a s
----

If the system high availability is functioning normally, the configured floating IP address will be displayed in the command output. For example, in the following command output, **inet 172.16.0.10/24 scope global secondary eth0** indicates that the floating IP address is bound to the eth0 network interface card:

// Bash
[source,bash]
----
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0
       valid_lft 73334sec preferred_lft 73334sec
    inet 172.16.0.10/24 scope global secondary eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
----
--

. Execute the following command to simulate a failure of the current load balancer server:
+
--
// Bash
[source,bash]
----
systemctl stop haproxy
----
--

. Execute the following command to check the floating IP address again:
+
--
// Bash
[source,bash]
----
ip a s
----

If the system high availability is functioning normally, the floating IP address will no longer be displayed in the command output, as shown in the following command output:

// Bash
[source,bash]
----
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0
       valid_lft 72802sec preferred_lft 72802sec
    inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
----
--

. Log in to the other load balancer server and execute the following command to view the floating IP address:
+
--
// Bash
[source,bash]
----
ip a s
----

If the system high availability is functioning normally, the configured floating IP address will be displayed in the command output. For example, in the following command output, **inet 172.16.0.10/24 scope global secondary eth0** indicates that the floating IP address is bound to the eth0 network interface card:

// Bash
[source,bash]
----
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 52:54:9e:3f:51:ba brd ff:ff:ff:ff:ff:ff
    inet 172.16.0.3/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0
       valid_lft 72690sec preferred_lft 72690sec
    inet 172.16.0.10/24 scope global secondary eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::f67c:bd4f:d6d5:1d9b/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
----
--

. Execute the following command on the first load balancer server to restore HAProxy operation:
+
--
// Bash
[source,bash]
----
systemctl start haproxy
----
--