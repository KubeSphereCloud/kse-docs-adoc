---
title: "Offline Installation of Kubernetes and KubeSphere"
keywords: "Kubernetes, {ks_product}, Installation, Install KubeSphere, Install Kubernetes"
description: "Describes how to install Kubernetes and KubeSphere."
weight: 02
draft: true
---

This section describes how to install Kubernetes and KubeSphere offline. The deployment environment does not require internet access, allowing for a completely offline installation of KubeSphere and extensions.

The open-source tool KubeKey will be used during the installation. For more information about KubeKey, please visit the link:https://github.com/kubesphere/kubekey[GitHub KubeKey repository].


== Prerequisites

* You need to contact KubeSphere delivery service experts to obtain the KubeSphere v4.1.3 installation package.

* You need to prepare at least 1 Linux server as a cluster node. In a production environment, to ensure high availability for the cluster, it is recommended to prepare at least 5 Linux servers, with 3 serving as control plane nodes and the other 2 as worker nodes. If you are installing KubeSphere on multiple Linux servers, ensure all servers belong to the same subnet.

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-desc-systemRequirements.adoc[]

* In a production environment, to ensure the cluster has sufficient computing and storage resources, it is recommended to configure each cluster node with at least 8 CPU cores, 16 GB of memory, and 200 GB of disk space. Additionally, it is recommended to mount at least 200 GB of additional disk space on each cluster node under the **/var/lib/docker** (for Docker) or **/var/lib/containerd** (for containerd) directory for storing container runtime data.

* Besides the cluster nodes, you need to prepare an additional Linux server for creating a private image registry. This server must be network-reachable from the KubeSphere cluster nodes and have at least 100 GB of disk space mounted under the **/mnt/registry** directory.

* In a production environment, it is recommended to configure high availability for the KubeSphere cluster in advance to avoid service interruption when a single control plane node fails. For more information, please refer to link:../../../03-installation-and-upgrade/01-preparations/02-configure-high-availability/02-configure-k8s-high-availability/[Configure High Availability].
+
--
// Note
include::../../../../_ks_components/admonitions/note.adoc[]

If you plan to have multiple control plane nodes, be sure to configure high availability for the cluster in advance.

include::../../../../_ks_components/admonitions/admonEnd.adoc[]
--

* By default, KubeSphere uses the local disk space of cluster nodes as persistent storage. In a production environment, it is recommended to configure an external storage system as persistent storage in advance. For more information, please refer to link:../../../03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/[Configure External Persistent Storage].

* If container runtimes are not installed on the cluster nodes, the installation tool KubeKey will automatically install Docker as the container runtime on each cluster node during the installation process. You can also manually install containerd, CRI-O, or iSula as the container runtime in advance.
+
--
// Note
include::../../../../_ks_components/admonitions/note.adoc[]

The compatibility of CRI-O and iSula with KubeSphere has not been fully tested and may have unknown issues.

include::../../../../_ks_components/admonitions/admonEnd.adoc[]
--

* Ensure the DNS server addresses configured in the **/etc/resolv.conf** file on all cluster nodes are reachable. Otherwise, the KubeSphere cluster may experience domain name resolution issues.

* Ensure the **sudo**, **curl**, and **openssl** commands are available on all cluster nodes.

* Ensure time synchronization across all cluster nodes.


== Configure Firewall Rules

KubeSphere requires specific ports and protocols for communication between services. If your infrastructure environment has a firewall enabled, you need to allow the required ports and protocols in the firewall settings. If your infrastructure environment does not have a firewall enabled, you can skip this step.

The following table lists the ports and protocols that need to be allowed in the firewall.

[%header,cols="1a,1a,1a,1a,2a"]
|===
|Service |Protocol |Start Port |End Port |Remarks

|ssh
|TCP
|22
|
|

|etcd
|TCP
|2379
|2380
|

|apiserver
|TCP
|6443
|
|

|calico
|TCP
|9099
|9100
|

|bgp
|TCP
|179
|
|

|nodeport
|TCP
|30000
|32767
|

|master
|TCP
|10250
|10258
|

|worker
|TCP
|10250
|
|

|dns
|TCP
|53
|
|

|dns
|UDP
|53
|
|

|local-registry
|TCP
|5000
|
|Required for offline environments

|local-apt
|TCP
|5080
|
|Required for offline environments

|rpcbind
|TCP
|111
|
|Required when using NFS as persistent storage

|ipip
|IPENCAP/IPIP
|
|
|Required when using Calico
|===

== Install Dependencies

You need to install socat, conntrack, ebtables, and ipset for all cluster nodes. If these dependencies already exist on the cluster nodes, you can skip this step.

On Ubuntu operating systems, execute the following command to install dependencies for the server:

// Bash
include::../../../../_ks_components/code/bash.adoc[]

sudo apt install socat conntrack ebtables ipset -y

include::../../../../_ks_components/code/codeEnd.adoc[]

If the cluster nodes use other operating systems, replace **apt** with the corresponding package management tool for that operating system.

== Verify Installation Package Integrity

After obtaining the KubeSphere installation package, you can verify its integrity by checking the file hash value to ensure it has not been corrupted.


. Depending on the operating system where the installation package file is located, execute the corresponding command to view the SHA256 value of the installation package.
+
====
* Windows System
+
--
[,bash]
----
certutil -hashfile [file location] SHA256
----

Example:
[,bash]
----
certutil -hashfile C:\Users\user1\Downloads\kse-v4.1.3-offline-linux-amd64-all-20250225.tar.gz SHA256
----
--

* Linux System
+
--
[,bash]
----
sha256sum [file location]
----

Example:
[,bash]
----
sha256sum ~/Downloads/kse-v4.1.3-offline-linux-amd64-all-20250225.tar.gz
----
--

* MacOS System
+
--
[,bash]
----
shasum -a 256 [file location]
----

Example:
[,bash]
----
shasum -a 256 ~/Downloads/kse-v4.1.3-offline-linux-amd64-all-20250225.tar.gz
----
--
====

. Compare the SHA256 value obtained in the previous step with the SHA256 value in the installation package to ensure they are consistent.
+
--
[.admon.attention,cols="a"]
|===
|Attention

|If the SHA256 values do not match, you need to re-obtain the complete KubeSphere installation package.
|===
--

== View Installation Package Contents

Understand the contents of the KubeSphere v4.1.3 installation package to proceed with the subsequent steps.

The installation package contains the following files:
[,bash]
----
kse-v4.1.3-offline-linux-amd64-all/
├── charts
│   ├── ks-core                   # KubeSphere core components
│   └── nfs-client-provisioner    # For integrating with NFS storage
├── tools
│   ├── extension-resources-patch.sh   # Used to handle conflicts caused by extension resources created via kse-extension-publish in versions v4.1.2 and earlier
│   └── oras                      # OCI tool, facilitates operations like image synchronization
├── kse-extensions                # Contains installplans for all extensions, can be used for quick installation of KubeSphere extensions
├── config-sample.yaml            # Template for the installation configuration file
├── create_project_harbor.sh      # Used to quickly create a harbor project
├── kk                            # Cluster deployment tool
├── kubekey-artifact.tar.gz       # KubeSphere artifacts, containing binaries and images required for cluster deployment
└── manifest-v413-amd64.yaml      # KubeSphere artifact manifest, containing component versions and image lists
----



== Configure the Installation Configuration File

**config-sample.yaml** is the installation configuration file for KubeSphere. Please configure this file first to proceed with the subsequent steps.

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-note-doNotDeleteConfig_v4.adoc[]

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-oper-decompressInstallationPackage_new.adoc[]
+

. Execute the following command to edit the installation configuration file **config-sample.yaml**:
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

vi config-sample.yaml

include::../../../../_ks_components/code/codeEnd.adoc[]

The following is a partial example configuration file. For the complete example, please refer to link:https://github.com/kubesphere/kubekey/blob/master/docs/config-example.md[this file].

// YAML
include::../../../../_ks_components/code/yaml.adoc[]

apiVersion: kubekey.kubesphere.io/v1alpha2
kind: Cluster
metadata:
  name: sample
spec:
  hosts:
  - {name: controlplane1, address: 192.168.0.2, internalAddress: 192.168.0.2, port: 23, user: ubuntu, password: Testing123, arch: arm64} # For arm64 nodes, add the parameter arch: arm64
  - {name: controlplane2, address: 192.168.0.3, internalAddress: 192.168.0.3, user: ubuntu, privateKeyPath: "~/.ssh/id_rsa"}
  - {name: worker1, address: 192.168.0.4, internalAddress: 192.168.0.4, user: ubuntu, password: Testing123}
  - {name: worker2, address: 192.168.0.5, internalAddress: 192.168.0.5, user: ubuntu, password: Testing123}
  - {name: registry, address: 192.168.0.6, internalAddress: 192.168.0.6, user: ubuntu, password: Testing123}
  roleGroups:
    etcd:
    - controlplane1
    - controlplane2
    control-plane:
    - controlplane1
    - controlplane2
    worker:
    - worker1
    - worker2
    # If you want to use kk to automatically deploy the image registry, set registry (it is recommended to deploy the image registry separately from cluster nodes to reduce mutual impact)
    registry:
    - registry
  controlPlaneEndpoint:
    internalLoadbalancer: haproxy # If deploying a high-availability cluster and no load balancer is available, you can enable this parameter for internal cluster load balancing
    domain: lb.kubesphere.local
    address: ""
    port: 6443
  kubernetes:
    version: v1.23.15
    clusterName: cluster.local
  network:
    plugin: calico
    kubePodsCIDR: 10.233.64.0/18
    kubeServiceCIDR: 10.233.0.0/18
    ## multus support. https://github.com/k8snetworkplumbingwg/multus-cni
    enableMultusCNI: false
  registry:
    # If you want to use kk to deploy harbor, set this parameter to harbor. If this parameter is not set and you need to use kk to deploy a container image registry, docker registry will be deployed by default.
    # harbor does not support arm64. When deploying in an arm64 environment, you may not configure this parameter.
    type: harbor
    # If using a harbor deployed by kk or another registry that requires login, you need to set the auths for the corresponding registry. If using the default docker registry deployed by kk, the auths parameter does not need to be configured.
    # Note: If using kk to deploy harbor, please set the auths parameter after creating the harbor project.
    auths:
      "dockerhub.kubekey.local":
        username: admin # Default harbor username
        password: Harbor12345 # Default harbor password
        plainHTTP: false  # If the registry uses http, set this parameter to true
    privateRegistry: "dockerhub.kubekey.local/kse"   # Set the private registry address used during cluster deployment
    registryMirrors: []
    insecureRegistries: []
  addons: []

include::../../../../_ks_components/code/codeEnd.adoc[]
--

. In the **spec:hosts** parameter of the **config-sample.yaml** configuration file, set the information for each server.
+
--
include::../../../_custom/installationAndUpgrade/installationAndUpgrade-para-hosts.adoc[]

--

. In the **spec:roleGroups** parameter of the **config-sample.yaml** configuration file, set the roles for the servers:
+
--
include::../../../_custom/installationAndUpgrade/installationAndUpgrade-para-roleGroups.adoc[]
--

. If you have planned multiple control plane nodes, set the high availability information in the **spec:controlPlaneEndpoint** parameter of the **config-sample.yaml** configuration file.
+
--
include::../../../_custom/installationAndUpgrade/installationAndUpgrade-para-controlPlaneEndpoint.adoc[]
--
. If you need to use external persistent storage, configure the external persistent storage information under the **spec:addons** parameter in the **config-sample.yaml** configuration file.
+
====
* If using cloud storage devices, set the following parameters under **spec:addons** (replace `<configuration file path>` with the actual path to the storage plugin configuration file):
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

  - name: csi-qingcloud
    namespace: kube-system
    sources:
      chart:
        name: csi-qingcloud
        path: charts/csi-qingcloud  # Replace with the actual path to the chart package
        valuesFile: <configuration file path>
include::../../../../_ks_components/code/codeEnd.adoc[]
--

* If using NeonSAN storage devices, set the following parameters under **spec:addons** (replace `<configuration file path>` with the actual path to the storage plugin configuration file):
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

  - name: csi-neonsan
    namespace: kube-system
    sources:
      chart:
        name: csi-neonsan
        path: charts/csi-neonsan  # Replace with the actual path to the chart package
        valuesFile: <configuration file path>

include::../../../../_ks_components/code/codeEnd.adoc[]
--

* If using an NFS storage system, set the following parameters under **spec:addons** (replace `<configuration file path>` with the actual path to the storage plugin configuration file):
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

  - name: nfs-client
    namespace: kube-system
    sources:
      chart:
        name: nfs-client-provisioner
        repo: charts/nfs-client-provisioner
        valuesFile: <configuration file path>

include::../../../../_ks_components/code/codeEnd.adoc[]
--
====

== Create a Private Image Registry

[.admon.attention,cols="a"]
|===
|Note

|
If you already have an available image registry, you can skip this step. However, you need to replace the default address of the private image service **dockerhub.kubekey.local/kse** with your actual image registry address.
|===

. In the **spec:hosts** parameter of the configuration file **config-sample.yaml**, set the information for the server used to create the private image service.
+
--
[,yaml]
----
spec:
  hosts:
  - {name: registry, address: 192.168.0.6, internalAddress: 192.168.0.6, user: ubuntu, password: Testing123}
----

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-para-hosts.adoc[]
--

. Under the **spec:roleGroups:registry** parameter, set the server name used to create the private image service (replace `<registry name>` with the actual server name set under the **spec:hosts** parameter).
+
[,yaml]
----
spec:
  roleGroups:
    registry:
    - <registry name>
----

. Set the **spec:registry:privateRegistry** parameter to the default address of the private image service **dockerhub.kubekey.local/kse**, then save the file.
+
[,yaml]
----
spec:
  registry:
    registryMirrors: []
    insecureRegistries: []
    privateRegistry: dockerhub.kubekey.local/kse
----


. Execute the following command to initialize the private image service:
+
====
[,bash]
----
./kk init registry -f config-sample.yaml -a kubekey-artifact.tar.gz
----

If the following information is displayed, it indicates that the image registry was created successfully.

image:/images/ks-qkcp/zh/v4.1/verify-registry.png[verify-registry, 100%]

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-note-initializeRegistry.adoc[]

====

. If the **spec:registry:type** parameter is set to **harbor**, execute the following command to create a Harbor project.
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

bash create_project_harbor.sh

include::../../../../_ks_components/code/codeEnd.adoc[]

After creating the harbor project, configure the **spec:registry:auths** parameter in **config-sample.yaml**.

[.admon.note,cols="a"]
|===
|Note

|
The harbor installation files are in the /opt/harbor directory. You can perform maintenance on harbor in this directory.

|===
--


. If all the following conditions are met, you need to perform the following steps to add containerd configuration on the node where harbor is located.
+
--
* The image registry uses harbor

* The container runtime is containerd

* The node where harbor is located is also within the cluster to be deployed

[source,bash]
----
mkdir /etc/containerd
----

[source,bash]
----
# Create the containerd configuration file
# Note to modify the sandbox_image and the repository information under [plugins."io.containerd.grpc.v1.cri".registry.configs]
# to the actual environment information

cat <<EOF > /etc/containerd/config.toml
version = 2
root = "/var/lib/containerd"
state = "/run/containerd"
 
[grpc]
  address = "/run/containerd/containerd.sock"
  uid = 0
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
 
[ttrpc]
  address = ""
  uid = 0
  gid = 0
 
[debug]
  address = ""
  uid = 0
  gid = 0
  level = ""
-----
[metrics]
  address = ""
ifeval::["{file_output_type}" == "pdf"]
----
<<<
[source,bash]
----
endif::[]
  grpc_histogram = false

[cgroup]
  path = ""

[timeouts]
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[plugins]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
    runtime_type = "io.containerd.runc.v2"
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
      SystemdCgroup = true
  [plugins."io.containerd.grpc.v1.cri"]
    sandbox_image = "dockerhub.kubekey.local/kse/kubesphere/pause:3.6"
    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      max_conf_num = 1
      conf_template = ""
    [plugins."io.containerd.grpc.v1.cri".registry]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
          endpoint = ["https://registry-1.docker.io"]
        [plugins."io.containerd.grpc.v1.cri".registry.configs]
          [plugins."io.containerd.grpc.v1.cri".registry.configs."dockerhub.kubekey.local".auth]
            username = "admin"
            password = "harbor2345"
            [plugins."io.containerd.grpc.v1.cri".registry.configs."dockerhub.kubekey.local".tls]
              ca_file = ""
              cert_file = ""
              key_file = ""
              insecure_skip_verify = true
EOF
----

[source,bash]
----
# Restart containerd
systemctl restart containerd
----
--
+

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-oper-editHosts_v4.adoc[]

== Install Kubernetes

// Note
include::../../../../_ks_components/admonitions/note.adoc[]

* If you already have a usable Kubernetes cluster, you can skip this step.

* The installation package includes dependency packages for CentOS 7, Ubuntu 18.04, Ubuntu 20.04, and Ubuntu 22.04. If using these operating systems and you need kk to automatically install system dependencies, you can add `--with-packages` to the installation command. If using an operating system other than these or if the installation fails due to dependency issues, you need to manually install the relevant dependencies (conntrack).

* If you need to use openebs localpv, you can add the parameter `--with-local-storage` to the following command. If you need to connect to other storage, you can add the configuration for the relevant storage plugin in the `addons` section of the configuration file, or install it yourself after the Kubernetes cluster deployment is complete.

* If using harbor deployed by kk, please ensure that the harbor project has been created before installing Kubernetes, and the `spec:registry:auths` parameter has been configured in the configuration file `config-sample.yaml`.

include::../../../../_ks_components/admonitions/admonEnd.adoc[]

Execute the following command to create the Kubernetes cluster:

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-startInstallationOffline.adoc[]

If the following information is displayed, it indicates that the Kubernetes cluster has been created successfully.

[,yaml]
----
Pipeline[CreateclusterPipeline] execute successfully
Installation is complete.
----


== Import Images to Private Image Registry

Execute the following command to import images to the specified private image registry.

[,bash]
----
./kk artifact images push  -f config-sample.yaml -a kubekey-artifact.tar.gz
----

If the following information is displayed, it indicates that the import was successful.

[,yaml]
----
Pipeline[ArtifactImagesPushPipeline] execute successfully
----

== Install KubeSphere

. On a cluster node, execute the following command to install KubeSphere Core.
+
====
[,bash]
----
helm upgrade --install -n kubesphere-system --create-namespace ks-core charts/ks-core \
--debug \
--wait \
--set global.imageRegistry=dockerhub.kubekey.local/kse \
--set extension.imageRegistry=dockerhub.kubekey.local/kse
----

[.admon.attention,cols="a"]
|===
|Note

|
Replace the default address **dockerhub.kubekey.local/kse** for **global.imageRegistry** and **extension.imageRegistry** with your actual image registry address.
|===

Depending on your hardware and network environment, you may need to configure traffic forwarding rules and open port 30880 in the firewall. If the following information is displayed, it indicates that ks-core has been installed successfully:

[,yaml]
----
NOTES:
Thank you for choosing KubeSphere Helm Chart.

Please be patient and wait for several seconds for the KubeSphere deployment to complete.

1. Wait for Deployment Completion

    Confirm that all KubeSphere components are running by executing the following command:

    kubectl get pods -n kubesphere-system

2. Access the KubeSphere Console

    Once the deployment is complete, you can access the KubeSphere console using the following URL:

    http://192.168.6.10:30880

3. Login to KubeSphere Console

    Use the following credentials to log in:

    Account: admin
    Password: P@88w0rd
NOTE: It is highly recommended to change the default password immediately after the first login.

For additional information and details, please visit https://kubesphere.io.
----
====

. From the success message, obtain the KubeSphere web console IP address, administrator username, and administrator password from the **Console**, **Account**, and **Password** parameters respectively. Then, log in to the KubeSphere web console using a web browser.

. According to the activation prompt, click **Activate** to import the KubeSphere license.

ifeval::["{file_output_type}" == "html"]
. At this point, the web console only provides the core functions of KubeSphere. To experience more features, you need to install extensions in the Extensions Center link:../../../06-extension-management/01-install-extensions/[install extensions].

endif::[]

ifeval::["{file_output_type}" == "pdf"]
. At this point, the web console only provides the core functions of KubeSphere. To experience more features, you need to install extensions in the Extensions Center. For more information, please refer to the *KubeSphere Extension Management Guide*.

endif::[]