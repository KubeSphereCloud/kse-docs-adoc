---
title: "Add Cluster Nodes"
keywords: "Kubernetes, {ks_product}, Add Cluster Nodes"
description: "Describes how to add KubeSphere cluster nodes."
weight: 01
---


This section describes how to add KubeSphere cluster nodes.

The open-source tool KubeKey will be used during the node addition process. For more information about KubeKey, please visit the link:https://github.com/kubesphere/kubekey[GitHub KubeKey repository].

// Note
include::../../../../_ks_components/admonitions/note.adoc[]

The node addition method described in this section is only applicable to scenarios where Kubernetes was installed via KubeKey. If your Kubernetes was not installed via KubeKey, please refer to the link:https://kubernetes.io/zh/docs/concepts/architecture/nodes/[Kubernetes official documentation] to add nodes.

include::../../../../_ks_components/admonitions/admonEnd.adoc[]


== Prerequisites

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-desc-systemRequirements.adoc[]

* To ensure the cluster has sufficient computing and storage resources, it is recommended that the new node has at least 8 CPU cores, 16 GB of memory, and 200 GB of disk space. In addition, it is recommended to mount at least 200 GB of additional disk space in the **/var/lib/docker** (for Docker) or **/var/lib/containerd** (for containerd) directory on each cluster node for storing container runtime data.

* If adding a control plane node, you need to configure high availability for the cluster in advance. If you are using a load balancer, ensure the load balancer listens on port 6443 of all control plane nodes. For more information, please refer to link:../../../03-installation-and-upgrade/01-preparations/02-configure-high-availability/02-configure-k8s-high-availability/[Configure High Availability].

// * If your cluster nodes cannot connect to the internet, you also need to prepare a Linux server to create a private image service. This server must be network-accessible to the KubeSphere cluster nodes and have at least 100 GB of disk space mounted in the **/mnt/registry** directory.

* socat, conntrack, tar, ebtables, and ipset are installed on the new node.

== If KubeSphere is Installed Online

If KubeSphere was link:../../02-install-kubesphere/01-online-install-kubernetes-and-kubesphere/[installed online], follow the steps below to add cluster nodes.

=== Preparations

You need to obtain the installation configuration file **config-sample.yaml** and transfer it to the cluster node used to perform the operations in this section.

include::../../../../_ks_components/admonitions/warning.adoc[]

* Modifying the original cluster configuration in the **config-sample.yaml** configuration file is not supported during node addition.

* If you cannot obtain the installation configuration file **config-sample.yaml**, you need to refer to link:../../02-install-kubesphere/01-online-install-kubernetes-and-kubesphere/[Install KubeSphere Online] to recreate the **config-sample.yaml** file. When recreating this file, be sure to ensure the cluster information in the file matches the current actual state of the cluster. Otherwise, errors may occur in the cluster after adding nodes.
include::../../../../_ks_components/admonitions/admonEnd.adoc[]


=== Steps


include::../../../_custom/installationAndUpgrade/installationAndUpgrade-oper-downloadKubekey.adoc[]

+
include::../../../_custom/installationAndUpgrade/installationAndUpgrade-oper-transferConfig.adoc[]

+

. Execute the following command to edit the installation configuration file **config-sample.yaml**:
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

vi config-sample.yaml

include::../../../../_ks_components/code/codeEnd.adoc[]
--

. Set the information for the new node under the **hosts** parameter in the **config-sample.yaml** file.
+
--

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-para-hosts.adoc[]

// Warning
include::../../../../_ks_components/admonitions/warning.adoc[]

Do not modify the information of the original nodes. Otherwise, errors may occur in the cluster after adding nodes.

include::../../../../_ks_components/admonitions/admonEnd.adoc[]
--

. Set the role of the new node in the cluster under the **roleGroups** parameter in the **config-sample.yaml** file.
+
--

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-para-roleGroups.adoc[]

// Warning
include::../../../../_ks_components/admonitions/warning.adoc[]

Do not modify the roles of the original nodes. Otherwise, errors may occur in the cluster after adding nodes.

include::../../../../_ks_components/admonitions/admonEnd.adoc[]
--

. If adding a control plane node and the current cluster is not configured for high availability, set the high availability information under the **controlPlaneEndpoint** parameter in the **config-sample.yaml** file.
+
--

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-para-controlPlaneEndpoint.adoc[]

// Warning
include::../../../../_ks_components/admonitions/warning.adoc[]

* If the current cluster is already configured for high availability, do not modify the high availability information in the **config-sample.yaml** file. Otherwise, errors may occur in the cluster after adding nodes.

* If the current cluster uses local load balancing for high availability, you do not need to perform any operations on cluster high availability; if the current cluster uses a load balancer for high availability, you only need to set the load balancer to listen on port 6443 of all control plane nodes. For more information, please refer to link:../../../03-installation-and-upgrade/01-preparations/02-configure-high-availability/02-configure-k8s-high-availability/[Configure High Availability].

include::../../../../_ks_components/admonitions/admonEnd.adoc[]
--

. Save the configuration file and execute the following command to start adding nodes:
+
--
// include::../../../_custom/installationAndUpgrade/installationAndUpgrade-code-addNodes.adoc[]
[source,bash]
----
Â ./kk add nodes -f config-sample.yaml
----
--

. Execute the following command to view the current cluster nodes:
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

kubectl get node

include::../../../../_ks_components/code/codeEnd.adoc[]

If the information of the new node is displayed, it indicates the node addition was successful.
--


== If KubeSphere is Installed Offline

If KubeSphere was link:../../02-install-kubesphere/02-offline-install-kubernetes-and-kubesphere/[installed offline] using the Web Installer, follow the steps below to add cluster nodes.

[.admon.note,cols="a"]
|===
|Note

|Adding nodes on the Web Installer page is not currently supported.
|===

=== Preparations

include::../../../_custom/installationAndUpgrade/installationAndUpgrade-oper-getInfoForOfflineInstall.adoc[]

=== Steps

. On the `deployment node`, navigate to the directory where the offline package is extracted (e.g., offline), and add the information of the new node to the `kkv4-inventory.yaml` file.
+
====
[source,bash]
----
cd offline
----

[source,bash]
----
vi kkv4-inventory.yaml
----

After the original node information in the `spec.host` field, add the information of the node to be added. Example:

[source,yaml]
----
spec:
  hosts:
    scan-cqxbav3: # Original node
      connector: 
        host: 172.16.0.4
        password: ""
        port: "22"
        user: "root"
      internal_ipv4: 172.16.0.4
      kubernetes:
        custom_labels: {}
    node1: # Name of the new node
      connector: # Node connection information
        host: 172.16.0.5 # SSH connection address of the node
        password: "" # SSH connection password for the node
        private_key_content: "" # SSH private key content for the node
        port: "" # SSH connection port for the node
        user: "" # SSH connection user for the node
      internal_ipv4: 172.16.0.5 # Internal communication IP of the node
      kubernetes:
        custom_labels: {} # Custom labels set for the node
----

Add the name of the new node to the `hosts` list under the corresponding node role in the `spec.groups` field. Example:

[source,yaml]
----
spec:
  groups:
    etcd:
      hosts:
      - scan-cqxbav3
      vars: null
    image_registry:
      hosts: []
      vars: null
    k8s_cluster:
      groups:
      - kube_control_plane
      - kube_worker
      hosts: null
      vars: null
    kube_control_plane:
      hosts:
      - scan-cqxbav3
      vars: null
    kube_worker:
      hosts:
      - scan-cqxbav3
      - node1 # Name of the new node
      vars: null
    nfs:
      hosts: []
      vars: null
----

* `spec.groups.etcd`: Add the node as an etcd node. Not currently supported.
* `spec.groups.kube_control_plane`: Add the node as a Kubernetes cluster control plane node.
* `spec.groups.worker`: Add the node as a Kubernetes cluster worker node.
====

. Use `kkv4-inventory.yaml` and `kkv4-config.json` to add nodes.
+
====
Replace <nodename1> <nodename2> with the node names added in kkv4-inventory.yaml.

[source,bash]
----
./kk add nodes <nodename1> <nodename2> -i kkv4-inventory.yaml -c kkv4-config.json
----

If the following information is displayed, it indicates the node addition was successful.

Example output:

// Bash
include::../../../../_ks_components/code/bash.adoc[]

[Playbook default/add-nodes-p44cg] finish. total: 229,success: 219,ignored: 10,failed: 0

include::../../../../_ks_components/code/codeEnd.adoc[]

====

. Log in to the `cluster node` and execute the following command to view the current cluster nodes:
+
--
// Bash
include::../../../../_ks_components/code/bash.adoc[]

kubectl get node

include::../../../../_ks_components/code/codeEnd.adoc[]

If the information of the new node is displayed, it also indicates the node addition was successful.
--